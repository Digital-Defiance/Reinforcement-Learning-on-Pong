{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from model import DQN\n",
    "from environment import PongEnvironment\n",
    "from replayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions\n",
    "def update_dqn(model, target_model, optimizer, batch, replay_buffer, gamma, losses):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    actions = actions.long()\n",
    "    states = states.view(states.size(0), -1)\n",
    "    next_states = next_states.view(next_states.size(0), -1)\n",
    "\n",
    "    q_values = model(states).gather(1, actions.view(-1, 1)).squeeze(1)\n",
    "    next_q_values = target_model(next_states).max(dim=1)[0].detach()\n",
    "\n",
    "    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "    loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss, q_values.mean().item()\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model, action_size):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(action_size)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "            q_values = model(state)\n",
    "            return q_values.argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "target_update_freq = 1000\n",
    "learning_rate = 0.001\n",
    "num_episodes = 10000\n",
    "losses = []\n",
    "avg_q_values = []\n",
    "\n",
    "# Initializing environments\n",
    "env = PongEnvironment()\n",
    "replay_buffer = ReplayBuffer()\n",
    "state_size = 6\n",
    "action_size = 2\n",
    "batch_size = 64\n",
    "model = DQN(state_size)\n",
    "target_model = DQN(state_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"trained_model.pth\"):\n",
    "    checkpoint = torch.load(\"trained_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    losses = checkpoint['losses']\n",
    "    avg_q_values = checkpoint['avg_q_values']\n",
    "    total_num_episodes = checkpoint['total_num_episodes']\n",
    "    if os.path.isfile('buffer.pkl'):\n",
    "        replay_buffer.load_buffer()\n",
    "    model.train()\n",
    "else:\n",
    "    print(\"TRAINING FOR FIRST TIME!\")\n",
    "    total_num_episodes = 0\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Training Loop\n",
    "epsilon = epsilon_start\n",
    "total_reward = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state = env.get_striker_and_ball_coordinates()\n",
    "    episode_q_values = []\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon, model, action_size)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        next_state = env.get_striker_and_ball_coordinates()\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = replay_buffer.sample_batch()\n",
    "            loss, avg_q_value = update_dqn(model, target_model, optimizer, batch, replay_buffer, gamma, losses)\n",
    "            losses.append(loss.item())\n",
    "            episode_q_values.append(avg_q_value)\n",
    "\n",
    "        if reward == 1 or reward == -1:\n",
    "            done = True\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        # env.render()\n",
    "        \n",
    "    epsilon = max(epsilon_end, epsilon * 0.995)\n",
    "    print(f\"Episode {episode + 1} : Reward = {reward}\")\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    if episode_q_values:\n",
    "        avg_q_values.append(np.mean(episode_q_values))\n",
    "\n",
    "print(\"---------------\")\n",
    "print(\"Total reward -> \", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and other metrics using checkpoints\n",
    "PATH = \"trained_model.pth\"\n",
    "TOTAL_NUM_EPISODES = num_episodes + total_num_episodes\n",
    "\n",
    "replay_buffer.save_buffer()\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'losses': losses,\n",
    "    'total_num_episodes': TOTAL_NUM_EPISODES,\n",
    "    'avg_q_values': avg_q_values\n",
    "}, PATH)\n",
    "\n",
    "print(\"Total number of episodes on which model is trained: \", TOTAL_NUM_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(avg_q_values)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
